{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, Pix2StructForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = \"data/images/\"\n",
    "QA_PATH = \"data/qa/\"\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"google/matcha-chartqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealCQA(Dataset):\n",
    "    def __init__(self, img_list) -> None:\n",
    "        super().__init__()\n",
    "        self.img_list = img_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item_id = self.img_list[idx][:-3]\n",
    "\n",
    "        # Get image with following name\n",
    "        image = Image.open(IMAGE_PATH + item_id + 'jpg')\n",
    "        \n",
    "        # Get corresponding json file\n",
    "        with open(QA_PATH + item_id + 'json', encoding='utf8') as f:\n",
    "            qa = json.load(f)\n",
    "        \n",
    "        # Since every image has a plethora of questions, select one from them randomly\n",
    "        rnd_sample = np.random.randint(len(qa))\n",
    "\n",
    "        # Take only question and corresponding answer from dict\n",
    "        q, a = qa[rnd_sample]['question'], qa[rnd_sample]['answer']\n",
    "\n",
    "        if isinstance(a, list):\n",
    "            while isinstance(a[0], list):\n",
    "                a = a[0]\n",
    "            a = ', '.join([str(el) for el in a])\n",
    "        \n",
    "        elif isinstance(a, int) or isinstance(a, float):\n",
    "            a = str(a)\n",
    "        \n",
    "        # Process images and correcponding questions\n",
    "        inputs = processor(images=image, text=q, return_tensors=\"pt\", max_patches=768).to(device)\n",
    "        \n",
    "        # Tokenize answers\n",
    "        inputs['labels'] = processor.tokenizer.encode(a, return_tensors=\"pt\", add_special_tokens=True, max_length=20, truncation=True, padding=\"max_length\").to(device)\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_list = os.listdir(IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = RealCQA(imgs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pix2StructForConditionalGeneration.from_pretrained(\"google/matcha-chartqa\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5000\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "      labels = batch.pop(\"labels\").to(device).squeeze(1)\n",
    "      flattened_patches = batch.pop(\"flattened_patches\").to(device).squeeze(1)\n",
    "      attention_mask = batch.pop(\"attention_mask\").to(device).squeeze(1)\n",
    "\n",
    "      outputs = model(flattened_patches=flattened_patches,\n",
    "                      attention_mask=attention_mask,\n",
    "                      labels=labels)\n",
    "      \n",
    "      loss = outputs.loss\n",
    "\n",
    "      print(\"Loss:\", loss.item())\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    predictions = model.generate(flattened_patches=flattened_patches, attention_mask=attention_mask)        \n",
    "    print(\"Predictions:\", processor.batch_decode(predictions, skip_special_tokens=True))\n",
    "    print(\"Ground-truth:\", processor.batch_decode(labels, skip_special_tokens=True))\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = batch.pop(\"labels\").to(device).squeeze(1)\n",
    "flattened_patches = batch.pop(\"flattened_patches\").to(device).squeeze(1)\n",
    "attention_mask = batch.pop(\"attention_mask\").to(device).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Илья\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: ['no', 'no']\n",
      "Ground-truth: ['yes', 'no']\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions = model.generate(flattened_patches=flattened_patches, attention_mask=attention_mask)        \n",
    "print(\"Predictions:\", processor.batch_decode(predictions, skip_special_tokens=True))\n",
    "print(\"Ground-truth:\", processor.batch_decode(labels, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
