{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, Pix2StructForConditionalGeneration\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from train import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = \"data/images/\"\n",
    "QA_PATH = \"data/qa/\"\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"google/matcha-chartqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealCQA(Dataset):\n",
    "    def __init__(self, img_list) -> None:\n",
    "        super().__init__()\n",
    "        self.img_list = img_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item_id = self.img_list[idx][:-3]\n",
    "\n",
    "        # Get image with following name\n",
    "        image = Image.open(IMAGE_PATH + item_id + 'jpg')\n",
    "        \n",
    "        # Get corresponding json file\n",
    "        with open(QA_PATH + item_id + 'json', encoding='utf8') as f:\n",
    "            qa = json.load(f)\n",
    "        \n",
    "        # Since every image has a plethora of questions, select one from them randomly\n",
    "        rnd_sample = np.random.randint(len(qa))\n",
    "\n",
    "        # Take only question and corresponding answer from dict\n",
    "        q, a = qa[rnd_sample]['question'], qa[rnd_sample]['answer']\n",
    "\n",
    "        if isinstance(a, list):\n",
    "            while isinstance(a[0], list):\n",
    "                a = a[0]\n",
    "            a = ', '.join([str(el) for el in a])\n",
    "        \n",
    "        elif isinstance(a, int) or isinstance(a, float):\n",
    "            a = str(a)\n",
    "        \n",
    "        # Process images and correcponding questions\n",
    "        inputs = processor(images=image, text=q, return_tensors=\"pt\", max_patches=768).to(DEVICE)\n",
    "        \n",
    "        # Tokenize answers\n",
    "        inputs['labels'] = processor.tokenizer.encode(a, return_tensors=\"pt\", add_special_tokens=True, max_length=20, truncation=True, padding=\"max_length\").to(DEVICE)\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_list = os.listdir(IMAGE_PATH)\n",
    "\n",
    "train_imgs, test_imgs = train_test_split(imgs_list, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = RealCQA(train_imgs)\n",
    "test_ds = RealCQA(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pix2StructForConditionalGeneration.from_pretrained(\"google/matcha-chartqa\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Илья\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\neptune\\common\\warnings.py:71: NeptuneWarning: The following monitoring options are disabled by default in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', and 'capture_hardware_metrics'. To enable them, set each parameter to 'True' when initializing the run. The monitoring will continue until you call run.stop() or the kernel stops. Also note: Your source files can only be tracked if you pass the path(s) to the 'source_code' argument. For help, see the Neptune docs: https://docs.neptune.ai/logging/source_code/\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/bng215/Model-Collapse/e/TRAN-981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: Train stage:   0%|          | 0/4947 [00:01<?, ?it/s]\n",
      "c:\\Users\\Илья\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: transmission loss r7(dB)\n",
      "Ground-truth: Line chart\n",
      "Line chart\n",
      "transmission loss r7(dB)\n",
      "Predictions: [Time, µs, 0.05]\n",
      "Ground-truth: Line chart\n",
      "Line chart\n",
      "[Time, µs, 0.05]\n",
      "Predictions: [PH, PH+3, PH+4, PH+5]\n",
      "Ground-truth: Line chart\n",
      "Line chart\n",
      "[PH, PH+3, PH+4, PH+5]\n",
      "Line chart\n",
      "[Moo, ZrMoO, SnMoO]\n",
      "Line chart\n",
      "[N-doped NIO, Non-doped NIO]\n",
      "Scatter chart\n",
      "[D1, D21]\n",
      "yes\n",
      "75\n",
      "yes\n",
      "No\n",
      "Vertical\n",
      "0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Илья\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified\n",
      "Communication\n",
      "Line chart\n",
      "0.5\n",
      "no\n",
      "No\n",
      "1.363961841841517\n",
      "0.215277778\n",
      "Line chart\n",
      "[Ag (40 nm) + ZnO (18 nm)]\n",
      "Vertical Boxplots\n",
      "4\n",
      "Line chart\n",
      "[Leaf height (cm) | Millenium | Maximum]\n",
      "Line chart\n",
      "7\n",
      "Vertical Bar chart\n",
      "[Runs, Conversion]\n",
      "Line chart\n",
      "[Flood risk reduction, Flood damage function of Condition 1, Flood damage function of Condition\n",
      "no\n",
      "No\n",
      "no\n",
      "No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Илья\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "No\n",
      "yes\n",
      "No\n",
      "Temperature of explosive graphitization\n",
      "10000/7, 1E-5\n",
      "no\n",
      "Yes\n",
      "Line chart\n",
      "[NeQ-50-5, NeQ-100-5]\n",
      "Line chart\n",
      "[Wavelength (nm)]\n",
      "Line chart\n",
      "[VITO CASE, VITO CORE]\n",
      "yes\n",
      "No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Илья\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "No\n",
      "Line chart\n",
      "[L/500, L/660]\n",
      "Line chart\n",
      "[Neat, Neat]\n",
      "Line chart\n",
      "[Chart, Type of chart]\n",
      "no\n",
      "Yes\n",
      "Line chart\n",
      "[1 - Specificity, 1 - Specificity]\n",
      "yes\n",
      "Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Илья\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "29\n",
      "Scatter chart\n",
      "[PCA factor 1, PCA factor 2]\n",
      "no\n",
      "No\n",
      "Line chart\n",
      "[Anatase, Anatase_]\n",
      "Line chart\n",
      "[Plasma frequency, Plasma frequency]\n",
      "Vertical Bar chart\n",
      "[Wild-type cells IgG, Wild-type cells 1ug Ab, As\n",
      "Vertical Boxplots\n",
      "400\n",
      "Line chart\n",
      "[Fe,0,@APPTES]\n",
      "no\n",
      "No\n",
      "Linear\n",
      "No\n",
      "Vertical Bar chart\n",
      "[Hypertension, Female]\n",
      "0.0\n",
      "1600\n",
      "Vertical Boxplots\n",
      "Ashcroft score\n",
      "Line chart\n",
      "[Nine, T16]\n",
      "no\n",
      "No\n",
      "no\n",
      "Yes\n",
      "no\n",
      "1.6\n",
      "no\n",
      "No\n",
      "Line chart\n",
      "[Cubic-C, A, Sub-C]\n",
      "no\n",
      "Yes\n",
      "Line chart\n",
      "[Indent 1 on Matic, Indent 2 on Matic]\n",
      "3.903225806451538\n",
      "0.214588889\n",
      "3\n",
      "2\n",
      "Line chart\n",
      "[Energy [keV], Illual]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Илья\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "25\n",
      "Line chart\n",
      "[Ti, Cross-section depth (um)]\n",
      "no\n",
      "No\n",
      "Line chart\n",
      "0.100\n",
      "no\n",
      "No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Илья\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8831635710005992\n",
      "0.433333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Илья\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Илья\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2476242193863698\n",
      "0.05\n",
      "Line chart\n",
      "100\n",
      "Line chart\n",
      "[1, 20]\n",
      "Line chart\n",
      "[Balancing, Bounce, OFFS way]\n",
      "Line chart\n",
      "[As-blended, Heat-annealed]\n",
      "Line chart\n",
      "[Time, The forecasted value]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Илья\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "15\n",
      "no\n",
      "Yes\n",
      "no\n",
      "No\n",
      "Vertical Bar chart\n",
      "[iso31, Alks]\n",
      "Line chart\n",
      "[Ti-21, Ti-6]\n",
      "yes\n",
      "No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Илья\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "No\n",
      "Line chart\n",
      "[Length-for-age BOYS, Length-for-age BOYS]\n",
      "Vertical\n",
      "0.6\n",
      "Available, Used\n",
      "10\n",
      "Line chart\n",
      "[1, 2]\n",
      "no\n",
      "Yes\n",
      "Line chart\n",
      "[Temperature (°C), Temperature (°C)]\n",
      "Average Surface Roughness (um)\n",
      "1.05\n",
      "Line chart\n",
      "[Distance from core haplotype (Mbp)]\n",
      "Scatter chart\n",
      "[Charter, Measure Haines, Charter 2, Charter 3]\n",
      "Line chart\n",
      "[Cholera, OCC]\n",
      "Vertical Bar chart\n",
      "[Medium, High]\n",
      "Vertical\n",
      "76\n",
      "6\n",
      "4\n",
      "Vertical Boxplots\n",
      "[C, M, U]\n",
      "Line chart\n",
      "[1, 2, 3]\n",
      "no\n",
      "Yes\n",
      "Line chart\n",
      "[SELFA2, SELFA3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Илья\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "2.4\n",
      "Vertical\n",
      "4\n",
      "Line chart\n",
      "[Composite 1, Composite 2]\n",
      "Vertical Bar chart\n",
      "[Stunting, Underweight, Wasting]\n",
      "Line chart\n",
      "[PTG-L8 B, PTG-X10 B]\n",
      "Linear\n",
      "No\n",
      "no\n",
      "No\n",
      "Line chart\n",
      "[B, C]\n",
      "Linear\n",
      "No\n",
      "Vertical Bar chart\n",
      "100\n",
      "Line chart\n",
      "[Multiples, Singletons]\n",
      "Line chart\n",
      "[20, 30]\n",
      "Line chart\n",
      "[BS882, PS89]\n",
      "Line chart\n",
      "[2e1, 211]\n",
      "Line chart\n",
      "[0, 10000, 20000]\n",
      "Line chart\n",
      "[1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Илья\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_{1}, t_{2}, t_{3}, \n",
      "2500\n",
      "Line chart\n",
      "0.5\n",
      "Line chart\n",
      "[Bastine, Week 12, Week 24]\n",
      "yes\n",
      "No\n",
      "yes\n",
      "Yes\n",
      "Scatter chart\n",
      "0\n",
      "Scatter chart\n",
      "[Worries (CW8 sum score), Worries (CW8 sum score\n",
      "yes\n",
      "Yes\n",
      "Line chart\n",
      "[Leaqua, Miro]\n",
      "Line chart\n",
      "[Follow-up in years, Cum-Survival]\n",
      "Vertical Bar chart\n",
      "[Anaemia, Mild anaemia]\n",
      "Line chart\n",
      "[PPTIN of S1, PPTIN of S2]\n",
      "Vertical\n",
      "4\n",
      "Line chart\n",
      "[BC-RU, BC-RU+BC [valDin]],[BC-RU\n",
      "Line chart\n",
      "[n+1, 10]\n",
      "Line chart\n",
      "[HEA-H, HEA-H]\n",
      "Line chart\n",
      "[1-Specificity, 1-Specificity]\n",
      "Vertical Bar chart\n",
      "[FMRI, TCD]\n",
      "Line chart\n",
      "[Brazil, Australia, Algeria, Mexico, United Kingdom]\n",
      "Line chart\n",
      "[Age at puberty (AGECL - Corrected)]\n",
      "Line chart\n",
      "[10, 40]\n",
      "no\n",
      "No\n",
      "no\n",
      "Yes\n",
      "\\Delta AUC (%)\n",
      "141\n",
      "yes\n",
      "Yes\n",
      "Line chart\n",
      "[Loadings, Loadings]\n",
      "Line chart\n",
      "[DAM, DAM vs. Vs. Spread]\n",
      "Horizontal Bar chart\n",
      "[UP,DOWN]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Илья\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "No\n",
      "Line chart\n",
      "[Mother died in 1st year, Mother did not die in 1st year]\n",
      "yes\n",
      "Yes\n",
      "Line chart\n",
      "[AS, YL]\n",
      "Line chart\n",
      "Engineering Strain\n",
      "Line chart\n",
      "17\n",
      "no\n",
      "Yes\n",
      "Line chart\n",
      "[JSON, JSON (GZip)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtest_dl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mneptune_tracking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Илья\\Desktop\\Sber-CV-Task\\train.py:123\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, train_dl, test_dl, num_epochs, processor, device, scheduler, neptune_tracking, start_epoch)\u001b[0m\n\u001b[0;32m    118\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(flattened_patches\u001b[38;5;241m=\u001b[39mflattened_patches,\n\u001b[0;32m    119\u001b[0m                 attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    120\u001b[0m                 labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[0;32m    122\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m--> 123\u001b[0m test_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss (Test)\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    125\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(flattened_patches\u001b[38;5;241m=\u001b[39mflattened_patches, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[0;32m    126\u001b[0m decoded_prediction \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mbatch_decode(predictions, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model=model,\n",
    "            optimizer=optimizer,\n",
    "            train_dl=train_dl,\n",
    "            test_dl=test_dl,\n",
    "            num_epochs=100,\n",
    "            processor=processor,\n",
    "            device=DEVICE,\n",
    "            scheduler=None,\n",
    "            neptune_tracking=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
